As a starting point, we adopt the objective maximum causal entropy IRL  (Eq.1.) Here we try to minimise, for a given cost function, we try to minimise the expected cost for a policy pi, while maintaining the entropy of the policy as high as possible. And then, for a fixed pi, we try to maximise the difference between this term and the expected cost for the expert demonstrations by updating the cost function.
If we choose to approximate the cost function with neural network, to be able to cover as much possible functions as possible, we will need a regulariser. With this, we add the regulariser to the original objective. 
Moreover, Proposition 3.2 in the paper shows that using the convex conjugate of the regulariser with  ro-pi minus ro-pi-expert gives us optimality guarantees that we can find a the optimal policy by optimising the Irl after RL problem directly. As the authors say, it becomes a saddle point of the  function, given by Eq.4. The regularisation in this formulation becomes very important meaning - we can solve the Rl after IRL problem directly, because in this formulation we seek for a policy whose  occupancy measure is close to the expert’s, as measured by the convex function ψ∗. (!!!!!! Main point of the paper)

As we see, the form of the occupancy measure matching is crucial. If psi is a constant, meaning no regularisation, the optimisation problem turns to maximising the negative entropy of pi, subject to the occupancy matching being equal to the expert occupancy matching, for any state-action pair, which might be a huge space. Here c(s, a) serve as dual variables.

Of course, the expert behaviour does not include samples of all state-action pairs if the environment  is sufficiently large. This means that we need some other measure of the difference of occupancy measures. For this purpose we can use the proposition above, making the convex conjugate psi star of the regularisation function the distance measure. For certain seatings of psi, the objective gets a familiar objective of apprenticeship learning. For this case, there are existing algorithms, alternating between fitting the cost fun ction to maximise the difference of the expected costs of simulated and expert trajectories. And in a second step update the policy with the current cost function. This method practically  scale to large environments. We will see that this is also the main idea behind the GAIL training. But the choice of psi in apprenticeship learning is not capable of recovering the true cost function. 

This leads to the proposed form of the regulariser psi, that can adapt to the expert data Eq. 13. But the exact choice of psi comes from the fact that the convex conjugate of psi for the occupancy matching is equal to the optimal negative log loss of a binary classifier that distinguishes between state acton pairs coming from the expert or from the current policy. This is also the connection to GAN, where the Discriminator has to distinguish between expert and generated data and the generator, that tries to generate examples that are indistinguishable for the discriminator from the expert examples.  Now we have the desired effect of avoiding to first learn a cost function, then a policy. As in GAN we gradually improve the examples generated by the policy until they are as good as the expert w.r.t. to a function with high capacity - the discriminator is a neural network. 

I'll not go into detail about trust region policy optimization, but you see here that this is a part of the GAIL learning process. It's simply a elaborated way updating the policy w.r.t. to the reward, as any policy gradient RL algorithm. For a fixed set of expert trajectories, we sample again and again simulated trajectories with the current policy pi. Then we update the discriminator to distinguish between the expert and the simulated samples. Then, using the score that the discriminator assigned to the simulated trajectories as reward, we update the policy parameters with TRPO or PPO. Normally in GAN we continue until the discriminator cannot distinguish between expert and simulated samples.
