As a starting point, we adopt the objective maximum causal entropy IRL  (Eq.1.) For the central equations I'll keep the numbering of the original paper. Maximum causal entropy IRL looks for a cost function c ∈ C that assigns low cost to the expert policy and high cost to other policies. Here for a given cost function, we try to minimise the expected cost w.r.t. a policy pi, while maintaining the entropy of the policy as high as possible. And then, for a fixed pi, we try to maximise the difference between this term and the expected cost for the expert demonstrations by updating the cost function.
So now we are looking in the set of all possible functions c. If we choose to approximate the cost function with neural network, to be able to cover as much possible functions as possible, we will need a regulariser to prevent overfitting . With this, we add the regulariser to the original objective. Now we can be sure that the cost function that explains the expert behaviour is among the solutions. We can use it to find the optimal policy for this cost function.
 What I skip here is the, but it's proven in the paper is the fact that policy pi and its occupancy measure ro_pi can be used interchangeably
Moreover, Proposition 3.2 in the paper shows that using the convex conjugate of the regulariser with  ro-pi minus ro-pi-expert gives us optimality guarantees that we can find a the optimal policy by optimising the Irl after RL problem directly. As the authors say, it becomes a saddle point of the  function, given by Eq.4. The regularisation in this formulation becomes very important meaning - we can solve the Rl after IRL problem directly, because in this formulation we seek for a policy whose  occupancy measure is close to the expert’s, as measured by the convex function ψ∗. (!!!!!! Main point of the paper). I got a snippet from the proof of this proposition to show where the convex conjugate comes into play. This is of course not the whole proof of the proposition, but here you can see the connection between the convex conjugate of psi and the objective function. And we have the convergence guarantees. 

As we see, the form of the occupancy measure matching is crucial. If psi is a constant, meaning no regularisation, the optimisation problem turns to maximising the negative entropy of pi, subject to the occupancy matching being equal to the expert occupancy matching, for any state-action pair. This might be a huge space and it just doesn't scale to large environments. Here the cost of a state-action pair serves as dual variables. Of course, the expert behaviour does not include samples of all state-action pairs if the environment  is sufficiently large. This means that we need some other measure of the difference of occupancy measures. 
For this purpose we can use the proposition above, making the convex conjugate psi star of the regularisation function the distance measure. For certain seatings of psi, the objective gets the known objective of apprenticeship learning, where the cost function is for example a linear combination of basis features. For this case, there are existing algorithms, alternating between fitting the cost function to maximise the difference of the expected costs of simulated and expert trajectories. And in a second step update the policy with the current cost function. This method practically  scale to large environments. We will see that this is also the main idea behind the GAIL training. But the choice of psi in apprenticeship learning is not capable of recovering the true cost function. 

This leads to the proposed form of the regulariser psi that at first looks a little scary. First of all, it can adapt to the expert data. But the exact choice of psi comes from the fact that the convex conjugate of psi for the occupancy matching is equal to the optimal negative log loss of a binary classifier that distinguishes between state acton pairs coming from the expert or from the current policy. This is also the connection to Generative adversarial nets, where the Discriminator has to distinguish between expert and generated data and the generator, that tries to generate examples that are indistinguishable for the discriminator from the expert examples.  Now we have the desired effect of avoiding to first learn a cost function, then a policy. As in GAN we gradually improve the examples generated by the policy until they are as good as the expert w.r.t. to a function with high capacity - in this case the discriminator is a neural network. 
 
I'll not go into detail about trust region policy optimization, but you see here that this is a part of the GAIL learning process. It's simply an elaborated way updating the policy w.r.t. to the reward, as any policy gradient RL algorithm. For a fixed set of expert trajectories, we sample again and again simulated trajectories with the current policy pi. Then we update the discriminator to distinguish between the expert and the simulated samples. Then, using the score that the discriminator assigned to the simulated trajectories as reward, we update the policy parameters with TRPO or PPO. Normally in GAN we continue until the discriminator cannot distinguish between expert and simulated samples.

I implemented the algorithm and applied it on the open gym Acrobot-v1 environment. I couldn't applied on the previous version, which it is not supported anymore. But there aren't severe differences. The acrobot system includes two joints and two links, where the joint between the two links is controlled by the agent. The goal is to swing the end of the lower link above the line.
The two main objects are the generator, which represents the policy and the discriminator. Both are torch nn.Modules. The discriminator is a simple MLP trained with binary cross entropy. If train it to predict score of one for the policy generated trajectories and zero for the expert we model exactly the step 4 from the algorithm - the one where we update the parameters of the  discriminator. For the training of the generator I took the PPO implementation from openAI and modified it to the task. Instead of generating samples and training directly, I first buffer the examples, then train the discriminator with them and then apply it to get the rewards for the transitions.  And we have an expert policy, which is a generator trained with the original reward, not the discriminator scores. We need it to generate the expert trajectories.

About the hyper parameters - the expert is trained until it doesn't improve the episode length anymore for which 50 epochs were enough. The other hyper parameters I took from the paper  

Performance measure is the episode length. For Acrobot: the lower - the better.

The results shown here are for the different expert dataset sizes. With the dataset of 10 expert trajectories the agent performs best and eventually, the policy becomes even better than the expert. The performance with the dataset with a single trajectory is similarly good, but still not improving to the expert. For seven trajectories the performance os similar to the one with 10. 

I also compare the performance of the same model but with the original reward of -1 everywhere. You see that the model needs much more time to grasp what to do until first finishing its episode. While the agent trained with expert trajectories is much faster in this. After that, both agents are relatively at the same speed of improvement.
I also find interesting to see how does the average scores from the discriminator look like. You see that they start at 0.5, since the parameters are random. Then at first the curves diverge, but then eventually get together again. The rapid improvement of the generator apparently sets the discriminator into a hard situation where the occupancy measures very fast become very similar to the expert. The loss also drops at the beginning but then starts increasing again. The scores oscillate for a while and then settle to 0.5. 
The results are comparable to the original paper.


I don't understand the oscillation of the loss function.

