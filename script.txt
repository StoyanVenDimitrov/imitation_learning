As a starting point, we adopt the objective maximum causal entropy IRL  (Eq.1.) Here we try to minimise, for a given cost function, we try to minimise the expected cost for a policy pi, while maintaining the entropy of the policy as high as possible. And then, for a fixed pi, we try to maximise the difference between this term and the expected cost for the expert demonstrations by updating the cost function.
If we choose to approximate the cost function with neural network, to be able to cover as much possible functions as possible, we will need a regulariser. With this, we add the regulariser to the original objective. 
Moreover, Proposition 3.2 in the paper shows that using the convex conjugate of the regulariser with  ro-pi minus ro-pi-expert gives us optimality guarantees that we can find a the optimal policy by optimising the Irl after RL problem directly. As the authors say, it becomes a saddle point of the  function, given by Eq.4. The regularisation in this formulation becomes very important meaning - we can solve the Rl after IRL problem directly, because in this formulation we seek for a policy whose  occupancy measure is close to the expert’s, as measured by the convex function ψ∗. (!!!!!! Main point of the paper)

Maybe say something about the case where Psi is constant - the max entropy IRL case of Ziebart.

As we see, the form of the occupancy measure matching is crucial. If psi is a constant, meaning no regularisation, the optimisation problem turns to maximising the negative entropy of pi, subject to the occupancy matching being equal to the expert occupancy matching, for any state-action pair, which might be a huge space.