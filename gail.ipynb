{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e841b4c8-5173-4bc9-895e-efe557f2efce",
   "metadata": {},
   "source": [
    "This notebook implements Generative Adversarial Imitation Learning(Ho and Ermon, 2016) on the basic OpenAI Gym environmet of Acrobot-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a58e8-99eb-419e-afc1-39df12055435",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning (GAIL)\n",
    "## Introduction\n",
    "I will make a brief introduction in the main components of reinforcement learning, as well as a short overview of imitation learning to make is more clear where GAIL can be situated and what is the motivation for it. \n",
    "\n",
    "Let's start with the basic tool to pose a reinforcement learning setting - the Markov Desicion Process(MDP). A MDP is a four-tuple $(S,A,P(s,a,s'), R(s,a,s'))$, where $S$ is the set of possible states, $A$ the state of possible action, $P(s,a,s')$ is the probability of transition in $s'$ after executing action $a$ when in state $s$, and $R(s,a,s')$ is the reward for this transtion. This way we can describe sequences of interactions with the environment under the use of the Markov property. The goal of the RL agent is to gather as much reward as possible in a long run. To achieve that in a sustainable way, the agent maintains a policy of what action to take when in a certain state. Mostly, we are interested in a stochastic policy - a function $\\pi: S \\to A$ in the form $\\pi(a|s)$. This represents the distribution over actions of what is the right action to do in the current state. Follwoing its goal, the agent aims to learn \n",
    "\n",
    "Reinforcement learning problems are formalised as Markov Decision Process (MDP). This framework can fully describe the components and interactions in RL, where the sequence of decisions is chosen with respect to the long-term effect on the total accumulated reward and not to the immediate ones. Formaly, a MDP consists of a n-tuple $\\{\\mathcal{S}, \\mathcal{A}, R, P, \\gamma \\}$. $\\mathcal{S}$ is the set of some discrete states. They are representation of the current environment's state. Based on them, the agent takes decision to act, executing action from the  set of some discrete actions $\\mathcal{A}$. The action sets the environment in a new state and responses with reward $R(s)$ for being at this state. P is the tensor of transitions probabilities $P(s,a,s') = p(s_{t+1}=s'|s_t=s, a_t=a)$, called also model of the environment, that fully specifies the outcome of taking action $a$ when in state $s$. When taking actions, the agent considers future rewards if might take along the way. How valuable these future rewards are for the current decision is  given by the discount factor $\\gamma$. The policy $\\pi(a|s)=Pr\\{A_t=a|S_t=s\\}$ expresses the preference of the agent for action $a$ when in state $s$. The final goal of the agent is to learn a policy that maximises the expected discounted return $G_t=\\sum_i^T \\gamma^i R_{t+i+1}$ from the current time step $t$ to the end of the episode at time $T$ (The same is true also for $T \\to \\infty$). The RL algorithms are focused in the task of finding this optimal policy $\\pi^*$.\n",
    "\n",
    "In imitation learning we might not have a reward function $R$, but additionally we have set of some demonstrations of desired(expert) behaviour, generated from an expert policy $\\pi_E$. We need to formalize a optimization task for the agent to be able to learn from the demonstrated behaviour. One way is to do Behaviour cloning, where we directly learn a policy by minimizing the KL divergence with the expert policy:$\n",
    "\\pi^L = argmin_{\\hat{\\pi}\\in\\Pi}\\mathbb{E}_{q(s,a)}\\left[\\ln\\pi_E(a|s)-\\ln\\hat{\\pi}(a|s)\\right]    \n",
    "$.\n",
    "Here we seek for the policy $\\hat{\\pi}$ that is most similar to the expert policy. Learning directly the policy of the demonstrator is sub-optimal: small differences in the environment responses at test time can lead to significant errors - if the agent enters a unknown region, the following behaviour will be sub-optimal. We need some way to introduce ordering over all trajectories in the MDP.\n",
    "\n",
    "With Inverse Reinforcement Learning (IRL) we try to learn the underlying reward/cost function $c \\in C$ that can explain the observations. We assume that the expert has an optimal policy w.r.t. to this function and if uncover it, then the agent can learn the optimal policy and this way be able to become as good as the expert in this task. Because the idea of uses Maximum causal entropy IRL as a starting point, let's look at its objective:\n",
    "\n",
    "$IRL(\\pi_E)=max_{c \\in C}(min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)])-\\mathbb{E}_{\\pi_E}[c(s,a)]$\n",
    "\n",
    "Maximum causal entropy IRL looks for a cost function $c \\in C$ that assigns low cost to the samples from expert policy and high cost to other policies. Here for a given cost function, we try to minimise the expected cost w.r.t. a policy pi, while maintaining the entropy of the policy as high as possible. And then, with the obtained $\\pi$, we try to maximise the difference between this term and the expected cost for the expert demonstrations by updating the cost function.\n",
    "\n",
    "## Motivaion\n",
    "Inverse Reniforcement Learning learns a cost function that explains the expert behaviour, but we still need to learn a policy after each change of the cost function. This is computationally expensive because we solve the RL problem in a inner loop again and again - for the objective above, we need to solve $RL(c)=argmin_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)]$. Then why to learn the cost function explicitly, if this doesn't tell the agent directly what to do? GAIL is one possible solution to learn a policy directly and avoid the intermediate IRL step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51a942-d78d-44cd-a873-79379a42e4f9",
   "metadata": {},
   "source": [
    "## The set of cost functions $C$\n",
    "Now we are looking in the set $C$ of all possible functions $c$ to be sure that the cost function that explains the expert behaviour is among the solutions. If we choose to approximate the cost function with neural network, for example, to be able to cover as much functions as possible, we will need a regulariser to avoid overfitting. The new, regularised objective becomes: \n",
    "\n",
    "$IRL_{\\psi}(\\pi_E)=max_{c \\in C} -\\psi(c) +(min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)])-\\mathbb{E}_{\\pi_E}[c(s,a)]$\n",
    "\n",
    "While the policy can be approximates by any function, also non-convex one like NN, the authors propose the transformation from policies $\\pi(a|s)$ to occupancy measures given by $\\rho_{\\pi}(s,a)= \\pi(a|s) \\sum_t \\gamma^t P(s_t=s|\\pi) $. When following a policy $\\pi$, this is the distribution of state-action pairs that the agent will encounter. The expected cost for this policy is then given using the occupancy measure by $\\mathbb{E}_{\\pi}[c(s,a)]=\\sum_{s,a} \\rho_{\\pi}(s,a)c(s,a)$.\n",
    "\n",
    "Proposition 3.1 in the original paper shows that for $\\pi_{\\rho}$ is the only policy whose occupancy measure is $\\rho$. Then we are allowed to re-write the cenrtal objective as follows:\n",
    "\n",
    "$IRL_{\\psi}(\\pi_E)=max_{c \\in C} -\\psi(c) +(min_{\\pi \\in \\Pi} -H(\\pi) + \\sum_{s,a} \\rho_{\\pi}(s,a)c(s,a))-\\sum_{s,a} \\rho_{\\pi_E}(s,a)c(s,a)$\n",
    "\n",
    "\n",
    "Moreover, Proposition 3.2 in the paper shows that using the convex conjugate of the regulariser with  $\\rho_{\\pi}-\\rho_{\\pi_E}$ gives us optimality guarantees that we can find a the optimal policy by optimising the $RL \\circ IRL_{\\psi}(\\pi_E)$ problem directly:\n",
    "$RL \\circ IRL_{\\psi}(\\pi_E) = argmin_{\\pi \\in \\Pi}-H(\\pi)+\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E})$ (Derived from the last equation and the definition of convex conjugate). The full proof relies on the fact that \n",
    "\n",
    "$L(\\rho, c)= -\\psi(c) -H(\\pi) + \\sum_{s,a} \\rho_{\\pi}(s,a)c(s,a))-\\sum_{s,a} \\rho_{\\pi_E}(s,a)c(s,a)$\n",
    "\n",
    "has a unique optimum $(\\rho^*, c^*)$ for it is true that $c^* \\in IRL_{\\psi}(\\pi_E)$ and $\\pi^* \\in RL \\circ IRL_{\\psi}(\\pi_E)$ with $\\pi_{\\rho^*}=\\pi^*$\n",
    "\n",
    "The regularisation in this formulation becomes very important meaning - we can solve the Rl after IRL problem directly, because in this formulation we seek for a policy whose  occupancy measure is close to the expertâ€™s, as measured by the convex function $\\psi^*$. \n",
    "\n",
    "This observation becomes the central motivation of GAIL. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ae11c-532b-4a27-bdc5-cb675716c8d5",
   "metadata": {},
   "source": [
    "## Using different kinds of $\\Psi$\n",
    "As we see, the form of the occupancy measure matching is crucial. If $\\psi$ is a constant, meaning no regularisation, the optimisation problem turns to maximising the $-H(\\pi)$, subject to $\\rho_{\\pi}(s,a) = \\rho_{\\pi_E}(s,a)$, for any state-action pair. This might be a huge space and it just doesn't scale to large environments. Here the cost of a state-action pair serves as dual variables. Of course, the expert behaviour does not include samples of all state-action pairs if the environment  is sufficiently large. This means that we need some other measure of the difference of occupancy measures. \n",
    "For this purpose we can use the proposition above, making the convex conjugate $\\psi^*$ of the regularisation function the distance measure. For certain seatings of $\\psi$, the objective gets the known objective of entropy-regularized apprenticeship learning, where the cost function is for example a linear combination of basis features: if we choose  $\\psi(c)=\\delta_C(c)$ where $\\delta_C(c)=0$ if $c \\in C$  and $\\infty$ otherwise. This means \n",
    "\n",
    "$min_{\\pi \\in \\Pi} -H(\\pi) + max_{c \\in C} \\mathbb{E}_{\\pi}[c(s,a)]-\\mathbb{E}_{\\pi_E}[c(s,a)] = min_{\\pi \\in \\Pi} -H(\\pi) + \\delta^*_C(\\rho_{\\pi}-\\rho_{\\pi_E})$\n",
    "\n",
    "For this case, there are existing algorithms, alternating between fitting the cost function to maximise the difference of the expected costs of simulated and expert trajectories. And in a second step update the policy with the current cost function. This method practically  scale to large environments. We will see that this is also the main idea behind the GAIL training. But if the true cost function $c^*$ is not in the set $C$ then the algorithm cannot recover the expert behaviour. The reason is given in the equation above - if $c^*$ in not in $C$, then is cannot be recovered by $RL \\circ IRL_{\\psi}(\\pi_E)$, which will then not have an optimal solution for the policy.\n",
    "\n",
    "This leads to the proposed form of the regulariser psi that at first looks a little scary. First of all, it can adapt to the expert data. But the exact choice of psi comes from the fact that the convex conjugate of psi for the occupancy matching is equal to the optimal negative log loss of a binary classifier that distinguishes between state acton pairs coming from the expert or from the current policy. This is also the connection to Generative adversarial nets, where the Discriminator has to distinguish between expert and generated data and the generator, that tries to generate examples that are indistinguishable for the discriminator from the expert examples.  Now we have the desired effect of avoiding to first learn a cost function, then a policy. As in GAN we gradually improve the examples generated by the policy until they are as good as the expert w.r.t. to a function with high capacity - in this case the discriminator is a neural network. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eeac28-1bd9-4ec8-942d-e8c8f35b9dac",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "I'll not go into detail about trust region policy optimization, but you see here that this is a part of the GAIL learning process. It's simply an elaborated way updating the policy w.r.t. to the reward, as any policy gradient RL algorithm. For a fixed set of expert trajectories, we sample again and again simulated trajectories with the current policy pi. Then we update the discriminator to distinguish between the expert and the simulated samples. Then, using the score that the discriminator assigned to the simulated trajectories as reward, we update the policy parameters with TRPO or PPO. Normally in GAN we continue until the discriminator cannot distinguish between expert and simulated samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739b319-d991-4157-9ffb-3dc54f063f8e",
   "metadata": {},
   "source": [
    "# Implementation \n",
    "## The environment \n",
    "I implemented the algorithm and applied it on the open gym Acrobot-v1 environment. I couldn't applied on the previous version, which it is not supported anymore. But there aren't severe differences. The acrobot system includes two joints and two links, where the joint between the two links is controlled by the agent. The goal is to swing the end of the lower link above the line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142700b6-9512-4e9e-a44b-357b6915ed93",
   "metadata": {},
   "source": [
    "GIF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a0314-7018-4ab3-a864-bf63012da831",
   "metadata": {},
   "source": [
    "## Get demonstrations\n",
    "\n",
    "To start the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637876e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to /tmp/experiments/1627023800/progress.txt\u001b[0m\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 11103, \t v: 10901\n",
      "\u001b[0m\n",
      "Warning: Log dir /tmp/experiments/1627023800 already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to /tmp/experiments/1627023800/progress.txt\u001b[0m\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 11103, \t v: 10901\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gail_obj = GAIL(\"Acrobot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb4199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########  Generating trajectories...  #########\n",
      "Batch 0\t Discriminator: loss: 1.4658763408660889\t expert mean 0.5193204283714294 \t generator mean 0.4826144874095917\n",
      "###########  Generating trajectories...  #########\n",
      "Batch 0\t Discriminator: loss: 1.4545559883117676\t expert mean 0.5212961435317993 \t generator mean 0.48978015780448914\n",
      "------------ Iteration 2 finished! ------------\n",
      "###########  Generating trajectories...  #########\n",
      "Batch 0\t Discriminator: loss: 1.448146939277649\t expert mean 0.523190975189209 \t generator mean 0.49444684386253357\n",
      "------------ Iteration 3 finished! ------------\n",
      "###########  Generating trajectories...  #########\n",
      "Batch 0\t Discriminator: loss: 1.4390614032745361\t expert mean 0.5250206589698792 \t generator mean 0.5005791783332825\n",
      "Warning: trajectory cut off by epoch at 48 steps.\n",
      "---------------------------------------\n",
      "|             EpLen |             495 |\n",
      "|             Epoch |               0 |\n",
      "|      AverageEpRet |            -251 |\n",
      "|          StdEpRet |            5.66 |\n",
      "|          MaxEpRet |            -234 |\n",
      "|          MinEpRet |            -254 |\n",
      "|      AverageVVals |          -0.106 |\n",
      "|          StdVVals |           0.165 |\n",
      "|          MaxVVals |           0.304 |\n",
      "|          MinVVals |          -0.435 |\n",
      "| TotalEnvInteracts |           5e+03 |\n",
      "|            LossPi |       -1.85e-07 |\n",
      "|             LossV |        1.79e+03 |\n",
      "|       DeltaLossPi |        -0.00527 |\n",
      "|        DeltaLossV |            -871 |\n",
      "|           Entropy |            1.09 |\n",
      "|                KL |         0.00673 |\n",
      "|          ClipFrac |           0.061 |\n",
      "|          StopIter |              79 |\n",
      "|              Time |            1.86 |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dimitrs/Development/stuttgart-university/imitation_learning/ppo/utils/mpi_tools.py:81: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  x = np.array(x, dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Iteration 4 finished! ------------\n",
      "###########  Generating trajectories...  #########\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x8/b4pbrcf93mxbp34r940r0y440000gn/T/ipykernel_2034/2418926146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgail_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/x8/b4pbrcf93mxbp34r940r0y440000gn/T/ipykernel_2034/225320397.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'###########  Generating trajectories...  #########'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mgen_trajectories_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_demonstrations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mgen_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_trajectories_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x8/b4pbrcf93mxbp34r940r0y440000gn/T/ipykernel_2034/225320397.py\u001b[0m in \u001b[0;36mget_demonstrations\u001b[0;34m(self, expert)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mflat_trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_policy_demonstrations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/x8/b4pbrcf93mxbp34r940r0y440000gn/T/ipykernel_2034/225320397.py\u001b[0m in \u001b[0;36m_generate_policy_demonstrations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mflat_trajectories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mnext_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/stuttgart-university/imitation_learning/ppo/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mlogp_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob_from_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/stuttgart-university/imitation_learning/ppo/core.py\u001b[0m in \u001b[0;36m_log_prob_from_distribution\u001b[0;34m(self, pi, act)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_log_prob_from_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/stuttgart-university/imitation_learning/venv/lib/python3.9/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/stuttgart-university/imitation_learning/venv/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument must be within the support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/stuttgart-university/imitation_learning/venv/lib/python3.9/site-packages/torch/distributions/constraints.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bound\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gail_obj.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explain the behaviour of the discriminator with the exploding gradients. Also, explain the vanishing gradients and the changed objecitve "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05f283d3629b4bcfd765d604275dad964af9f9304f0198b5b2824db52ea45b93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
