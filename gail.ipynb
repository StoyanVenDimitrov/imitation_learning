{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e841b4c8-5173-4bc9-895e-efe557f2efce",
   "metadata": {},
   "source": [
    "This notebook implements Generative Adversarial Imitation Learning(Ho and Ermon, 2016) on the basic OpenAI Gym environmet of Acrobot-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a58e8-99eb-419e-afc1-39df12055435",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning (GAIL)\n",
    "## Introduction\n",
    "I will make a brief introduction in the main components of reinforcement learning, as well as a short overview of imitation learning to make is more clear where GAIL can be situated and what is the motivation for it. \n",
    "\n",
    "Let's start with the basic tool to pose a reinforcement learning setting - the Markov Desicion Process(MDP). A MDP is a four-tuple $(S,A,P(s,a,s'), R(s,a,s'))$, where $S$ is the set of possible states, $A$ the state of possible action, $P(s,a,s')$ is the probability of transition in $s'$ after executing action $a$ when in state $s$, and $R(s,a,s')$ is the reward for this transtion. This way we can describe sequences of interactions with the environment under the use of the Markov property. The goal of the RL agent is to gather as much reward as possible in a long run. To achieve that in a sustainable way, the agent maintains a policy of what action to take when in a certain state. Mostly, we are interested in a stochastic policy - a function $\\pi: S \\to A$ in the form $\\pi(a|s)$. This represents the distribution over actions of what is the right action to do in the current state. Follwoing its goal, the agent aims to learn \n",
    "\n",
    "Reinforcement learning problems are formalised as Markov Decision Process (MDP). This framework can fully describe the components and interactions in RL, where the sequence of decisions is chosen with respect to the long-term effect on the total accumulated reward and not to the immediate ones. Formaly, a MDP consists of a n-tuple $\\{\\mathcal{S}, \\mathcal{A}, R, P, \\gamma \\}$. $\\mathcal{S}$ is the set of some discrete states. They are representation of the current environment's state. Based on them, the agent takes decision to act, executing action from the  set of some discrete actions $\\mathcal{A}$. The action sets the environment in a new state and responses with reward $R(s)$ for being at this state. P is the tensor of transitions probabilities $P(s,a,s') = p(s_{t+1}=s'|s_t=s, a_t=a)$, called also model of the environment, that fully specifies the outcome of taking action $a$ when in state $s$. When taking actions, the agent considers future rewards if might take along the way. How valuable these future rewards are for the current decision is  given by the discount factor $\\gamma$. The policy $\\pi(a|s)=Pr\\{A_t=a|S_t=s\\}$ expresses the preference of the agent for action $a$ when in state $s$. The final goal of the agent is to learn a policy that maximises the expected discounted return $G_t=\\sum_i^T \\gamma^i R_{t+i+1}$ from the current time step $t$ to the end of the episode at time $T$ (The same is true also for $T \\to \\infty$). The RL algorithms are focused in the task of finding this optimal policy $\\pi^*$.\n",
    "\n",
    "In imitation learning we might not have a reward function $R$, but additionally we have set of some demonstrations of desired(expert) behaviour, generated from an expert policy $\\pi_E$. We need to formalize a optimization task for the agent to be able to learn from the demonstrated behaviour. One way is to do Behaviour cloning, where we directly learn a policy by minimizing the KL divergence with the expert policy:$\n",
    "\\pi^L = argmin_{\\hat{\\pi}\\in\\Pi}\\mathbb{E}_{q(s,a)}\\left[\\ln\\pi_E(a|s)-\\ln\\hat{\\pi}(a|s)\\right]    \n",
    "$.\n",
    "Here we seek for the policy $\\hat{\\pi}$ that is most similar to the expert policy. Learning directly the policy of the demonstrator is sub-optimal: small differences in the environment responses at test time can lead to significant errors - if the agent enters a unknown region, the following behaviour will be sub-optimal. We need some way to introduce ordering over all trajectories in the MDP.\n",
    "\n",
    "With Inverse Reinforcement Learning (IRL) we try to learn the underlying reward/cost function $c \\in C$ that can explain the observations. We assume that the expert has an optimal policy w.r.t. to this function and if uncover it, then the agent can learn the optimal policy and this way be able to become as good as the expert in this task. Because the idea of uses Maximum causal entropy IRL as a starting point, let's look at its objective:\n",
    "\n",
    "$IRL(\\pi_E)=max_{c \\in C}(min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)])-\\mathbb{E}_{\\pi_E}[c(s,a)]$\n",
    "\n",
    "Maximum causal entropy IRL looks for a cost function $c \\in C$ that assigns low cost to the samples from expert policy and high cost to other policies. Here for a given cost function, we try to minimise the expected cost w.r.t. a policy pi, while maintaining the entropy of the policy as high as possible. And then, with the obtained $\\pi$, we try to maximise the difference between this term and the expected cost for the expert demonstrations by updating the cost function.\n",
    "\n",
    "We will also need a brief idea of Generative adversarial networks (GAN). GANs are a class of generative models in which a generator is trained to optimize a cost function that is being simultaneously updated by a discriminator distribution. The generator generated a sample, while the discriminator assigns a score to the generated sample about its belief that the sample comes from a demonstration set $\\mathcal{D}$. Thus, the discriminator's objective is:\n",
    "    $\\mathcal{L}(D) = \\mathbb{E}_{x\\sim \\mathcal{D}} [-\\log D(x)] + \\mathbb{E}_{x\\sim G} [-\\log(1- D(x))]$.\n",
    "The generator is optimized via the discriminator's confusion: \n",
    "$\\mathcal{L}(G) = \\mathbb{E}_{x\\sim G} [-\\log D(x)] + \\mathbb{E}_{x\\sim G} [\\log(1- D(x))]$.\n",
    "When the generator' loss becomes zero, this means that the discriminator is not able to decide where the sample comes from. In other words, the generator unveiled the underlying distribution of the demonstrations.\n",
    "\n",
    "## Motivaion\n",
    "Inverse Reniforcement Learning learns a cost function that explains the expert behaviour, but we still need to learn a policy after each change of the cost function. This is computationally expensive because we solve the RL problem in a inner loop again and again - for the objective above, we need to solve $RL(c)=argmin_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)]$. Then why to learn the cost function explicitly, if this doesn't tell the agent directly what to do? GAIL is one possible solution to learn a policy directly and avoid the intermediate IRL step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51a942-d78d-44cd-a873-79379a42e4f9",
   "metadata": {},
   "source": [
    "## The set of cost functions $C$\n",
    "Now we are looking in the set $C$ of all possible functions $c$ to be sure that the cost function that explains the expert behaviour is among the solutions. If we choose to approximate the cost function with neural network, for example, to be able to cover as much functions as possible, we will need a regulariser to avoid overfitting. The new, regularised objective becomes: \n",
    "\n",
    "$IRL_{\\psi}(\\pi_E)=max_{c \\in C} -\\psi(c) +(min_{\\pi \\in \\Pi} -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)])-\\mathbb{E}_{\\pi_E}[c(s,a)]$\n",
    "\n",
    "While the policy can be approximates by any function, also non-convex one like NN, the authors propose the transformation from policies $\\pi(a|s)$ to occupancy measures given by $\\rho_{\\pi}(s,a)= \\pi(a|s) \\sum_t \\gamma^t P(s_t=s|\\pi) $. When following a policy $\\pi$, this is the distribution of state-action pairs that the agent will encounter. The expected cost for this policy is then given using the occupancy measure by $\\mathbb{E}_{\\pi}[c(s,a)]=\\sum_{s,a} \\rho_{\\pi}(s,a)c(s,a)$.\n",
    "\n",
    "Proposition 3.1 in the original paper shows that for $\\pi_{\\rho}$ is the only policy whose occupancy measure is $\\rho$. Then we are allowed to re-write the cenrtal objective as follows:\n",
    "\n",
    "$IRL_{\\psi}(\\pi_E)=max_{c \\in C} -\\psi(c) +(min_{\\pi \\in \\Pi} -H(\\pi) + \\sum_{s,a} \\rho_{\\pi}(s,a)c(s,a))-\\sum_{s,a} \\rho_{\\pi_E}(s,a)c(s,a)$\n",
    "\n",
    "\n",
    "Moreover, Proposition 3.2 in the paper shows that using the convex conjugate of the regulariser with  $\\rho_{\\pi}-\\rho_{\\pi_E}$ gives us optimality guarantees that we can find a the optimal policy by optimising the $RL \\circ IRL_{\\psi}(\\pi_E)$ problem directly:\n",
    "$RL \\circ IRL_{\\psi}(\\pi_E) = argmin_{\\pi \\in \\Pi}-H(\\pi)+\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E})$ (Derived from the last equation and the definition of convex conjugate). The full proof relies on the fact that \n",
    "\n",
    "$L(\\rho, c)= -\\psi(c) -H(\\pi) + \\sum_{s,a} \\rho_{\\pi}(s,a)c(s,a))-\\sum_{s,a} \\rho_{\\pi_E}(s,a)c(s,a)$\n",
    "\n",
    "has a unique optimum $(\\rho^*, c^*)$ for it is true that $c^* \\in IRL_{\\psi}(\\pi_E)$ and $\\pi^* \\in RL \\circ IRL_{\\psi}(\\pi_E)$ with $\\pi_{\\rho^*}=\\pi^*$\n",
    "\n",
    "The regularisation in this formulation becomes very important meaning - we can solve the Rl after IRL problem directly, because in this formulation we seek for a policy whose  occupancy measure is close to the expertâ€™s, as measured by the convex function $\\psi^*$. \n",
    "\n",
    "This observation becomes the central motivation of GAIL. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ae11c-532b-4a27-bdc5-cb675716c8d5",
   "metadata": {},
   "source": [
    "## Using different kinds of $\\Psi$\n",
    "As we see, the form of the occupancy measure matching is crucial. If $\\psi$ is a constant, meaning no regularisation, the optimisation problem turns to maximising the $-H(\\pi)$, subject to $\\rho_{\\pi}(s,a) = \\rho_{\\pi_E}(s,a)$, for any state-action pair. This might be a huge space and it just doesn't scale to large environments. Here the cost of a state-action pair serves as dual variables. Of course, the expert behaviour does not include samples of all state-action pairs if the environment  is sufficiently large. This means that we need some other measure of the difference of occupancy measures. \n",
    "For this purpose we can use the proposition above, making the convex conjugate $\\psi^*$ of the regularisation function the distance measure. For certain seatings of $\\psi$, the objective gets the known objective of entropy-regularized apprenticeship learning, where the cost function is for example a linear combination of basis features: if we choose  $\\psi(c)=\\delta_C(c)$ where $\\delta_C(c)=0$ if $c \\in C$  and $\\infty$ otherwise. This means \n",
    "\n",
    "$min_{\\pi \\in \\Pi} -H(\\pi) + max_{c \\in C} \\mathbb{E}_{\\pi}[c(s,a)]-\\mathbb{E}_{\\pi_E}[c(s,a)] = min_{\\pi \\in \\Pi} -H(\\pi) + \\delta^*_C(\\rho_{\\pi}-\\rho_{\\pi_E})$\n",
    "\n",
    "For this case, there are existing algorithms, alternating between fitting the cost function to maximise the difference of the expected costs of simulated and expert trajectories. And in a second step update the policy with the current cost function. This method practically  scale to large environments. We will see that this is also the main idea behind the GAIL training. But if the true cost function $c^*$ is not in the set $C$ then the algorithm cannot recover the expert behaviour. The reason is given in the equation above - if $c^*$ in not in $C$, then is cannot be recovered by $RL \\circ IRL_{\\psi}(\\pi_E)$, which will then not have an optimal solution for the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c84f0-edde-47ba-acd3-78e60cea4cca",
   "metadata": {},
   "source": [
    "## Regularisation with GAIL\n",
    "\n",
    "This leads to the proposed form of the regulariser $\\psi$. \n",
    "![title](psi.png)\n",
    "\n",
    "First of all, it can adapt to the expert data.  But the exact choice of $\\psi$ comes from the fact that the convex conjugate of psi for the occupancy matching is equal to the optimal negative log loss of a binary classifier that distinguishes between state acton pairs coming from the expert or from the current policy.\n",
    "\n",
    "$\\psi^*(\\rho_{\\pi} - \\rho_{\\pi_E}) = max \\mathbb{E}_{\\pi}[log(D(s,a))] + \\mathbb{E}_{\\pi_E}[log(1-D(s,a))]$\n",
    "\n",
    "This is also the connection to Generative adversarial nets, where the discriminator has to distinguish between expert and generated data and the generator, that tries to generate examples that are indistinguishable for the discriminator from the expert examples.  Here we have the desired effect of avoiding to first learn a cost function, then a policy: as in GAN we gradually improve the examples generated by the policy until they are as good as the expert w.r.t. to a function with high capacity - in this case the discriminator is a neural network. \n",
    "\n",
    "!!! explain the generator and connect to PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eeac28-1bd9-4ec8-942d-e8c8f35b9dac",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "I'll not go into detail about trust region policy optimization, but it's simply an elaborated way updating the policy w.r.t. to the reward, as any policy gradient RL algorithm. For a fixed set of expert trajectories, we sample simulated trajectories with the current policy $\\pi$ parametrized by $\\theta$. In reality, we don't use whole trajectories of interactions, in order of appearance, but on a set of single interactions with the environment (state, action, reward, next state). Then we update the weights $\\omega$ of the discriminator to distinguish between the expert and the simulated samples. Using the score that the discriminator assigned to the simulated trajectories as reward, we update the policy parameters with TRPO or proximal policy optimization (PPO). We continue until the discriminator cannot distinguish between expert and simulated samples.\n",
    "\n",
    "![title](algo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739b319-d991-4157-9ffb-3dc54f063f8e",
   "metadata": {},
   "source": [
    "# Implementation \n",
    "\n",
    "## Requirements \n",
    "\n",
    "\n",
    "\n",
    "## The environment \n",
    "I implemented the algorithm and applied it on the open gym Acrobot-v1 environment. I couldn't applied on the previous version, used in the paper, which it is not supported anymore. But there aren't severe differences. The acrobot system includes two joints and two links, where the joint between the two links is controlled by the agent. The goal is to swing the end of the lower link above the line. For a visualisation, see the [original description](https://gym.openai.com/envs/Acrobot-v1/). The environment assignes a reward of -1 to every timestep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11c1a0c-1aac-4bb7-8ea5-bbd710199396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 19:54:20.477 Python[50163:6278634] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/x8/b4pbrcf93mxbp34r940r0y440000gn/T/org.python.python.savedState\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "env.seed(543)\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c877d52-39c5-4d50-9b9e-b34aa7c889da",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "I define the generator as a class. Its main purpose is to represent a policy with a neural network and be trained with PPO. The Generator class is mostly based on the [openai PPO implementation](https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/ppo). Since this is a big class with a lot of dependencies, I won't put them directly on this notebook. You can find it under generator.py. The main functionalities serve PPO, which is not in the scope of this project. \n",
    "The Generator maintains NN estimators for the policy and for the value function, since it is trained in Actor-Critic manner. These are packed in an object of the class core.MLPActorCritic. Each network has two hidden layers of 100 units an tanh activations, as pointed in the original paper. \n",
    "The Generator class has a ppo() method that is meant for training the policy. I updated it to also deal with the case where the reward where we generate interactions outside the method and apply the discriminator on them to obtain rewards, before starting PPO. I use the same method to train an expert for generating the expert demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dedaa-c4ca-4115-9b41-f44ff57047dd",
   "metadata": {},
   "source": [
    "## Discriminator \n",
    "\n",
    "The discriminator receives a state-action pair from interaction sample and puts a score on it, representing the believe that this is a sample, generated from the policy instead of an expert sample. If use Binary Cross-Entropy between scores for policy samples and 1 as label and scores for expert samples and 0 as label, we reconstruct the proposed discriminator objective of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156b78af-67b1-44f7-b152-bf6c3ac87a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"the discriminator class\n",
    "    \"\"\"\n",
    "    def __init__(self, state_shape, hidden_shape=100, learning_rate=0.0002):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "        input_shape = state_shape + 1\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_shape, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hidden_shape, out_features=hidden_shape, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=hidden_shape, out_features=1, bias=True), # final\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input.float())\n",
    "\n",
    "    def train(self, expert_trajectories, policy_trajectories):\n",
    "        \"\"\"train to distinguish expert from generated data\n",
    "         Args:\n",
    "            expert_trajectories (List)\n",
    "            policy_trajectories ([type])\n",
    "        Return:\n",
    "            error, mean of the predicted score for expert samples and same for the generator samples\n",
    "        \"\"\"\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        expert_output = torch.cat([self.forward(torch.cat((state, action))) \n",
    "                            for state, action in zip(expert_trajectories['state'], torch.unsqueeze(expert_trajectories['action'], 1))])\n",
    "        policy_output = torch.cat([self.forward(torch.cat((state, action))) \n",
    "                            for state, action in zip(policy_trajectories['state'], torch.unsqueeze(policy_trajectories['action'], 1))])\n",
    "\n",
    "        expert_labels = torch.zeros_like(expert_output, dtype=torch.float)\n",
    "        policy_labels = torch.ones_like(policy_output, dtype=torch.float)\n",
    "\n",
    "        errD_expert = criterion(expert_output, expert_labels)\n",
    "        errD_policy = criterion(policy_output, policy_labels)\n",
    "\n",
    "        errD_expert.backward()\n",
    "        errD_policy.backward()\n",
    "\n",
    "        errD = errD_expert + errD_policy\n",
    "        self.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            expert_output = torch.sigmoid(expert_output)\n",
    "            policy_output = torch.sigmoid(policy_output)\n",
    "\n",
    "        return errD.item(), torch.mean(expert_output).item(), torch.mean(policy_output).item(), expert_output, policy_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a0454e-b0ef-483d-ac29-41a2819b60d7",
   "metadata": {},
   "source": [
    "Let's initialize the main components and set some hyperparameters as in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d727c7b-0801-457c-b96f-c621043445e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1mLogging data to /tmp/experiments/1627063459/progress.txt\u001b[0m\n",
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 11103, \t v: 10901\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EXPERT_TRAJECTORIES = 7 # how many expert trajectories to use\n",
    "EXPERT_TRAJECTORIES_LIST = [1, 4, 7, 10] # (important for loading .json with expert demos)\n",
    "GENERATOR_TIMESTEPS = 5000 # how many env interactions to gather before updating the policy\n",
    "EXPERT_TRAIN_EPOCHS = 50 # when training the expert, how many epochs to learn\n",
    "ITERATIONS = 300 # num of training iterations (step 2 in algorithm)\n",
    "MAX_EP_LEN = 500 # if not done by MAX_EP_LEN, restart the environment for new episode\n",
    "DISC_ITER = 0   # num iteration of pre-training only the discriminator \n",
    "DISC_L_RATE = 0.0002 # discriminator Adam learning rate\n",
    "\n",
    "discriminator = Discriminator(state_shape=env.observation_space.shape[0], learning_rate=DISC_L_RATE)\n",
    "generator = Generator(env, discriminator, max_ep_len=MAX_EP_LEN, steps_per_epoch=GENERATOR_TIMESTEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a0314-7018-4ab3-a864-bf63012da831",
   "metadata": {},
   "source": [
    "## Get demonstrations\n",
    "\n",
    "To start the GAIL training, we first need expert demonstrations. For this purpose, we can train a policy with the original reward. Since the training takes a longer, I provide .json files with expert 1, 4, 7 and 10 expert trajectories, as used in the paper. The expert model is of class Generator, but it needs no Discriminator - it uses the -1 reward.  The method is also the same for generating policy demonstrations. If you really want to train a new model and get new expert trajectories, delete the .json files in the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8da9027-bf65-42b6-8141-17ac88f6529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json_tricks\n",
    "\n",
    "from utils import _generate_expert_demonstrations, _generate_policy_demonstrations\n",
    "\n",
    "def get_demonstrations(env, expert=False):\n",
    "    \"\"\"get demonstrations from an expert/policy model\n",
    "\n",
    "    Args:\n",
    "        expert (bool, optional): [are these demonstrations from an expert]. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        [torch.utils.data.DataLoader]: torch DataLoader object with the created data\n",
    "    \"\"\"\n",
    "    env_name = env.spec.id\n",
    "    if expert:\n",
    "        try:\n",
    "            with open(f'expert_{env_name}_demos_{EXPERT_TRAJECTORIES}.json', 'r') as fp:\n",
    "                flat_trajectories = json_tricks.load(fp)\n",
    "        except FileNotFoundError:\n",
    "            model = Generator(env, None)\n",
    "            print(EXPERT_TRAIN_EPOCHS)\n",
    "            model.ppo(epochs=EXPERT_TRAIN_EPOCHS)\n",
    "            for num_trajectories in EXPERT_TRAJECTORIES_LIST:\n",
    "                flat_trajectories = _generate_expert_demonstrations(model, env, num_trajectories)\n",
    "                with open(f'expert_{env_name}_demos_{num_trajectories}.json', 'w') as fp:\n",
    "                    json_tricks.dump(flat_trajectories, fp)\n",
    "        return flat_trajectories\n",
    "    if not expert:\n",
    "        return _generate_policy_demonstrations(generator, env, gen_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a7d52-24a1-4318-a030-4a653df3b660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05f283d3629b4bcfd765d604275dad964af9f9304f0198b5b2824db52ea45b93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
